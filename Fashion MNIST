from ctypes import sizeof
from tarfile import CompressionError
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda, Compose
import matplotlib.pyplot as plt

# Fashion MNIST데이터에서 학습 데이터를 불러옵니다.
training_data = datasets.FashionMNIST(
    root = "data",
    train = True,
    download = True,
    transform = ToTensor(),
)

# Fashion MNIST데이터에서 테스트데이터를 불러옵니다.
test_data = datasets.FashionMNIST(
    root = "data",
    train = False,
    download = True,
    transform = ToTensor(),
)

batch_size = 64

# 데이터 로더를 생성합니다.
train_dataloader = DataLoader(training_data, batch_size = batch_size)
test_dataloader = DataLoader(test_data, batch_size = batch_size)

for X, y in test_dataloader:
    print('Shape of X [N, C, H, W]', X.shape)
    print('Shape of y :', y.shape, y.dtype)
    break

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512,512),
            nn.ReLU(),
            nn.Linear(512, 512)
        )
        
    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

    def train(dataloader, model, loss_fn, optimizer):
        size = len(dataloader.dataset)
        for batch, (X,y) in enumerate(dataloader):
            X, y = X.to(device), y.to(device)
            
            #예측 오류 계산
            pred = model(X)
            loss = loss_fn(pred, y)
            
            #역전파
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if batch % 100 == 0:
                loss, current = loss.item(), batch *len(X)
                print(f"loss: {loss:7f} [{current:>5d}/{size:>5d}]")
                
    def test(dataloader, model, loss_fn):
        size = len(dataloader.dataset)
        num_batch = len(dataloader)
        model.eval()
        test_loss, correct = 0, 0
        with torch.no_grad():
            for X, y in dataloader:
                X, y = X.to(device), y.to(device)
                pred = model(X)
                test_loss += loss_fn(pred, y).item()
                correct += (pred.argmax(1) == y).type(torch.float).sum().item()
        test_loss /= num_batch
        correct /= size        
        print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
        
            
model = NeuralNetwork().to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)
epochs = 5

for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------")
    NeuralNetwork.train(train_dataloader, model, loss_fn, optimizer)
    NeuralNetwork.test(test_dataloader, model, loss_fn)
    print("Done!")
